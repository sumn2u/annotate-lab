
@inproceedings{dutta_via_2019,
	address = {Nice France},
	title = {The via annotation software for images, audio and video},
	isbn = {9781450368896},
	url = {https://dl.acm.org/doi/10.1145/3343031.3350535},
	doi = {10.1145/3343031.3350535},
	language = {en},
	urldate = {2024-06-13},
	booktitle = {Proceedings of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Dutta, Abhishek and Zisserman, Andrew},
	month = oct,
	year = {2019},
	pages = {2276--2279},
}

@misc{cvat_ai_corporation_computer_2024,
	title = {Computer vision annotation tool({Cvat})},
	copyright = {MIT License},
	url = {https://zenodo.org/doi/10.5281/zenodo.11543564},
	abstract = {Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale.},
	urldate = {2024-06-19},
	publisher = {CVAT.ai Corporation},
	author = {{CVAT. ai Corporation}},
	month = jun,
	year = {2024},
	doi = {10.5281/ZENODO.11543564},
	keywords = {image-labeling-tool, computer-vision-annotation, labeling-tool, image-labeling, semantic-segmentation, annotation-tool, object-detection, image-classification, video-annotation, computer-vision, deep-learning, annotation},
}


@inproceedings{ojha_image_2017,
	title = {Image annotation using deep learning: {A} review},
	shorttitle = {Image annotation using deep learning},
	url = {https://ieeexplore.ieee.org/document/8321819},
	doi = {10.1109/I2C2.2017.8321819},
	abstract = {In the last few years, deep learning has led to huge success in the field of computer vision and natural language understanding and also in the interplay between them. Among different types of deep learning models, convolutional neural networks have been most extensively studied for the tasks related to visual perception and machine vision. Due to lack of computational resources and training data, it is very hard to the use high-capacity convolutional neural network without overfitting. But recent growth in the availability of annotated data and high performance GPUs have made it possible to obtain state-of-the-art results using convolutional neural networks. In this paper, we present a review on how and why CNNs are extensively getting used in the computer vision community. It also introduces an application of ConvNets for annotating contents of the image by partially localizing them.},
	urldate = {2024-06-13},
	booktitle = {2017 {International} {Conference} on {Intelligent} {Computing} and {Control} ({I2C2})},
	author = {Ojha, Utkarsh and Adhikari, Utsav and Singh, Dushyant Kumar},
	month = jun,
	year = {2017},
	keywords = {Neurons, Convolutional neural networks, Computer architecture, Training, Computational modeling, Visualization, Task analysis, Deep Learning, Image annotation, Convolution Neural Network, ImageNet, Recurrent Nets},
	pages = {1--5},
}

@article{aljabri_towards_2022,
	title = {Towards a better understanding of annotation tools for medical imaging: a survey},
	volume = {81},
	issn = {1380-7501, 1573-7721},
	shorttitle = {Towards a better understanding of annotation tools for medical imaging},
	url = {https://link.springer.com/10.1007/s11042-022-12100-1},
	doi = {10.1007/s11042-022-12100-1},
	language = {en},
	number = {18},
	urldate = {2024-06-13},
	journal = {Multimedia Tools and Applications},
	author = {Aljabri, Manar and AlAmir, Manal and AlGhamdi, Manal and Abdel-Mottaleb, Mohamed and Collado-Mesa, Fernando},
	month = jul,
	year = {2022},
	pages = {25877--25911},
}

@article{liu_survey_2024,
	title = {A survey on autonomous driving datasets: statistics, annotation quality, and a future outlook},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {A survey on autonomous driving datasets},
	url = {https://arxiv.org/abs/2401.01454},
	doi = {10.48550/ARXIV.2401.01454},
	abstract = {Autonomous driving has rapidly developed and shown promising performance due to recent advances in hardware and deep learning techniques. High-quality datasets are fundamental for developing reliable autonomous driving algorithms. Previous dataset surveys either focused on a limited number or lacked detailed investigation of dataset characteristics. To this end, we present an exhaustive study of 265 autonomous driving datasets from multiple perspectives, including sensor modalities, data size, tasks, and contextual conditions. We introduce a novel metric to evaluate the impact of datasets, which can also be a guide for creating new datasets. Besides, we analyze the annotation processes, existing labeling tools, and the annotation quality of datasets, showing the importance of establishing a standard annotation pipeline. On the other hand, we thoroughly analyze the impact of geographical and adversarial environmental conditions on the performance of autonomous driving systems. Moreover, we exhibit the data distribution of several vital datasets and discuss their pros and cons accordingly. Finally, we discuss the current challenges and the development trend of the future autonomous driving datasets.},
	urldate = {2024-06-13},
	author = {Liu, Mingyu and Yurtsever, Ekim and Fossaert, Jonathan and Zhou, Xingcheng and Zimmer, Walter and Cui, Yuning and Zagar, Bare Luka and Knoll, Alois C.},
	year = {2024},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@article{kirillov_segment_2023,
	title = {Segment anything},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2304.02643},
	doi = {10.48550/ARXIV.2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2024-08-01},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll√°r, Piotr and Girshick, Ross},
	year = {2023},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
}